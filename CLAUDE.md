# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This project demonstrates fine-tuning small language models (Qwen3-0.6B) for reliable function calling using MLX LoRA, then integrating with the Model Context Protocol (MCP) for real-world tool execution. The key achievement is 100% accuracy on function calling tasks with a 600M parameter model.

## Architecture

### Three-Layer System

1. **Training Layer** (`train/`)
   - Converts Glaive function calling dataset to MLX chat format
   - Fine-tunes using LoRA adapters with 8 trainable layers
   - Dataset format: Glaive's `<functioncall>` tags with JSON payloads

2. **Parsing Layer** (`mcp/client.py`)
   - Model outputs Glaive format: `<functioncall> {"name": "func", "arguments": '{"param": value}'}`
   - Parser extracts function name and arguments using regex
   - Bridges model output format to MCP protocol

3. **MCP Integration** (`mcp/server.py`)
   - FastMCP server exposes real API tools (weather, currency)
   - MCP client discovers tools and executes via protocol
   - No retraining needed - any model format works with proper parser

### Key Insight: Format vs Protocol

- The model can output **any format** (Glaive, custom JSON, etc.)
- A **parser** bridges that format to MCP calls
- MCP is protocol-agnostic - it doesn't care about model output format
- This means you can fine-tune with Glaive format and still integrate with MCP

## Common Commands

### Development

```bash
# Run demo (loads both base and fine-tuned models, shows side-by-side comparison)
make demo
# or: uv run mcp/client.py

# Train the model (takes ~1 hour on Apple Silicon)
make train
# or: uv run train/train.py

# Install dependencies
make install
# or: uv sync

# Clean cache and generated JSONL files
make clean

# Remove all training artifacts (adapters, data)
make clean-all
```

### Training Configuration

Edit `train/config.yaml` before training. Key parameters:

- `iters`: Training iterations (default: 2500 for ~1 hour)
- `num_layers`: LoRA trainable layers (8 out of 24 total)
- `batch_size`: Memory optimization (1, with grad_accumulation_steps=4)
- `learning_rate`: Conservative 1e-5 for stable convergence
- `test: false` - Keep disabled (test evaluation takes 7-8 hours for 1k samples)

### Inspecting Training Data

The conversion from Glaive to MLX format happens in `train/train.py:convert_to_chat_format()`. To verify conversion:

```bash
# View first training example
head -1 train/data/train.jsonl | python -m json.tool
```

Critical bug fixed: Initial parser concatenated all messages into user role. Current implementation uses regex splitting on role markers (`USER:`, `ASSISTANT:`, `FUNCTION RESPONSE:`).

## File Structure

```
train/
├── train.py              # Dataset conversion + training orchestration
├── config.yaml           # MLX training config (hyperparameters, LoRA settings)
├── data/                 # Generated by train.py (40k examples, 100MB)
│   ├── train.jsonl       # MLX chat format
│   ├── valid.jsonl
│   └── test.jsonl
└── adapters/             # Fine-tuned LoRA weights (130MB)

mcp/
├── server.py             # FastMCP server with 2 tools (weather, currency)
└── client.py             # Demo: base vs fine-tuned comparison with MCP execution
```

## Important Patterns

### Adding New MCP Tools

In `mcp/server.py`, add a decorated function:

```python
@mcp.tool()
def your_new_tool(param: str) -> str:
    """Tool description shown to model in system prompt."""
    # Implementation with real API calls
    return result
```

The model automatically discovers new tools on next run via `mcp_client.refresh_tools()` in client.py:115.

### Function Call Parsing

Located in `mcp/client.py:parse_function_call()`. Handles Glaive format quirks:

- Arguments field is a JSON string with single quotes: `"arguments": '{"param": value}'`
- Uses regex for extraction (not JSON parsing) due to nested quoting
- Returns `{"name": str, "arguments": dict}` or `None`

### Model Loading

Two models are loaded in `mcp/client.py:main()`:

- Base model: `mlx_lm.load(MODEL_NAME)`
- Fine-tuned: `mlx_lm.load(MODEL_NAME, adapter_path=ADAPTER_PATH)`

Both use same tokenizer and system prompt building via `build_system_prompt(functions)`.

## Training Specifics

### Memory Optimizations (Apple Silicon)

```yaml
batch_size: 1                   # Minimal memory per step
grad_accumulation_steps: 4      # Same gradient quality as batch=4
grad_checkpoint: true           # 30-50% memory savings
num_layers: 8                   # Trainable layers (not all 24)
```

Peak memory: 5.5GB on M1/M2/M3

### LoRA Configuration

```yaml
lora_parameters:
  keys: [self_attn.q_proj, self_attn.v_proj, self_attn.k_proj, self_attn.o_proj]
  rank: 8              # Low-rank decomposition size
  scale: 20.0          # Alpha value (scale/rank = 2.5x)
  dropout: 0.0         # No dropout for stability
```

Trainable params: ~130MB adapters on top of 600MB base model

### Dataset Conversion Logic

Source: Glaive Function Calling v2 (112k examples)
Used: 40k train, 1k validation, 1k test

Conversion (`train/train.py:35-99`):
1. Extract system message (remove "SYSTEM: " prefix)
2. Split chat on role markers using regex: `\n\n+(?=(?:USER:|ASSISTANT:|FUNCTION RESPONSE:))`
3. Parse each segment by role
4. Skip FUNCTION RESPONSE (model learns to call, not simulate responses)
5. Remove `<|endoftext|>` markers

## Model Output Formats

This codebase uses **Glaive format** for training:

```
<functioncall> {"name": "get_weather", "arguments": '{"location": "Tokyo"}'}
```

Other formats you may encounter:

**Qwen Native** (Qwen2.5-3B has built-in support):
```
<tool_call>{"name": "...", "arguments": {"param": "value"}}</tool_call>
```

**OpenAI/Industry Standard**:
```json
{"role": "assistant", "tool_calls": [{"type": "function", ...}]}
```

When fine-tuning is necessary vs base model's native function calling, depends on whether the base model already supports your target format with acceptable accuracy.

## Troubleshooting

### Out of Memory During Training

- Reduce `batch_size` to 1 (already default)
- Ensure `grad_checkpoint: true` in config.yaml
- Reduce `num_layers` to 4
- Close other applications

### Poor Accuracy After Training

- Increase `iters` to 3000+
- Verify dataset conversion: `head -1 train/data/train.jsonl | python -m json.tool`
- Ensure `num_layers` is at least 8
- Increase `train_samples` to 40k+ in train.py

### Function Calls Not Detected in Demo

- Check parser regex in `mcp/client.py:parse_function_call()`
- Verify model outputs contain `<functioncall>` tags
- Enable prompt display: set `show_full_prompt=True` in client demo loop

### MCP Connection Issues

- Ensure `uv` is installed: `curl -LsSf https://astral.sh/uv/install.sh | sh`
- Server starts automatically via `stdio_client()` in client.py
- Check server script path in `mcp_client.connect("mcp/server.py")`

## Testing and Validation

The demo (`mcp/client.py`) runs 6 test queries against both base and fine-tuned models:
- Weather queries (Boston, Tokyo, Raleigh)
- Currency conversions (USD↔EUR, GBP→USD, CAD→EUR)
- Shows raw outputs, function detection, and MCP tool results
- Displays generation time and side-by-side comparison

Use this instead of MLX's built-in test evaluation which takes 7-8 hours.

## Performance Metrics (Reference)

- Training time: 1h 7m (40k samples, 2500 iterations)
- Memory usage: 5.5GB peak
- Inference latency: 1-2s per query
- Accuracy: 100% (12/12 test queries)
- Model size: 130MB LoRA adapters
- Final train loss: 0.614
- Final validation loss: 0.626
- Test perplexity: 1.697
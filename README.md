# Function Calling Agent with MCP Integration

Fine-tune small language models for reliable function calling, then integrate with the Model Context Protocol (MCP) for real-world tool execution.

## Overview

This project demonstrates:
- ✅ **MLX LoRA fine-tuning** on Apple Silicon
- ✅ **100% accuracy** on function calling tasks
- ✅ **MCP integration** - connect fine-tuned models to real APIs
- ✅ **No retraining needed** - Glaive format works with MCP
- ✅ **Production-ready** - complete training and inference pipeline

**Key Achievement:** Fine-tuned Qwen3-0.6B (600M params) achieves perfect function calling accuracy and seamlessly integrates with MCP servers.

## Quick Start

### Run the Demo

```bash
# Run MCP demo (loads both base and fine-tuned models, shows comparison)
uv run mcp/client.py
```

This will:
1. Load base and fine-tuned models
2. Connect to MCP server automatically
3. Run 4 test queries with real API calls (weather, currency)
4. Show side-by-side comparison of base vs fine-tuned performance

### Train Your Own Model

```bash
# Configure training in train/config.yaml, then run:
uv run train/train.py
```

Training takes ~1 hour on Apple Silicon (M1/M2/M3) with ~5.5GB peak memory.

## Project Structure

```
function-calling-agent/
├── README.md                # This file
├── pyproject.toml           # Package dependencies
├── mcp/                     # MCP integration
│   ├── server.py            # FastMCP server (2 real API tools)
│   └── client.py            # Demo with side-by-side comparison
└── train/                   # Training pipeline
    ├── train.py             # Training script with dataset conversion
    ├── config.yaml          # MLX training configuration
    ├── data/                # Training data (100MB, generated by train.py)
    │   ├── train.jsonl      # 40k training examples
    │   ├── valid.jsonl      # Validation set
    │   └── test.jsonl       # Test set
    └── adapters/            # Fine-tuned LoRA weights (130MB)
        ├── adapters.safetensors
        └── adapter_config.json
```

## How It Works

### Architecture

```
User Query: "What's the weather in Tokyo?"
    ↓
Fine-tuned Model → <functioncall> {"name": "get_weather", "arguments": '{"location": "Tokyo"}'}
    ↓
Parser → Extracts: {"name": "get_weather", "arguments": {"location": "Tokyo"}}
    ↓
MCP Client → Converts to MCP protocol
    ↓
MCP Server → Executes real API call
    ↓
Result: "Tokyo: 72°F (22°C), Partly cloudy"
```

### Key Insight: MCP ≠ Model Format

**MCP is a protocol**, not a model output format:
- Your model can output **ANY format** (Glaive, custom JSON, etc.)
- A **parser** bridges your format to MCP calls
- MCP handles tool discovery and execution
- No retraining needed to integrate with MCP!

## Training Guide

### Configuration

Edit `train/config.yaml` to configure training:

```yaml
# Key parameters (defaults work well):
model: "mlx-community/Qwen3-0.6B-4bit"    # Small model, perfect for fine-tuning
num_layers: 8                              # LoRA layers (out of 24 total)
batch_size: 1                              # Memory-efficient
grad_accumulation_steps: 4                 # Effective batch = 4
iters: 2500                                # ~1 hour training
learning_rate: 1e-5                        # Stable convergence
```

### Dataset

Uses [Glaive Function Calling v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) (112k examples):

**Glaive Format:**
```
<functioncall> {"name": "get_weather", "arguments": '{"location": "Tokyo"}'}
```

The training script automatically:
1. Downloads the dataset
2. Converts to MLX chat format
3. Splits into train/valid/test
4. Saves to `data/` directory

### Training Results

**Final Configuration (Iteration 4):**
- Training time: 1h 7m 34s
- Peak memory: 5.556 GB
- Samples: 40,000
- Iterations: 2,500
- Success rate: **12/12 (100%)**

**Metrics:**
- Final train loss: 0.614
- Final validation loss: 0.626
- Test loss: 0.529 (excellent generalization!)
- Test perplexity: 1.697

### Training Evolution

| Attempt | Layers | Iterations | Samples | Result |
|---------|--------|------------|---------|--------|
| 1 | 4 | 600 | 10k | Failed - wrong format |
| 2 | 4 | 600 | 10k | Failed - dataset bug |
| 3 | 8 | 1200 | 10k | 80% success |
| **4** | **8** | **2500** | **40k** | **100% success ✅** |

## MCP Integration

### Why MCP?

MCP (Model Context Protocol) provides:
- **Dynamic tool discovery** - model sees all available tools
- **Real API calls** - weather, currency, databases, etc.
- **Multiple servers** - connect to different tool ecosystems
- **Industry standard** - works with any MCP-compatible client

### Available Tools

The MCP server (`mcp/server.py`) exposes:

1. **get_weather(location)** - Real weather data via wttr.in
2. **convert_currency(amount, from, to)** - Real exchange rates

### Adding More Tools

Just add a decorated function to `mcp/server.py`:

```python
@mcp.tool()
def your_new_tool(param: str) -> str:
    """Tool description shown to the model."""
    # Your implementation
    return result
```

The model automatically discovers it on next run!

## Performance Comparison

### Base vs Fine-tuned Model

| Metric | Base Model | Fine-tuned Model |
|--------|------------|------------------|
| Function call detection | ❌ 0% | ✅ 100% |
| Format consistency | ❌ Poor | ✅ Perfect |
| Argument extraction | ❌ Unreliable | ✅ Accurate |
| Hallucinations | ❌ Frequent | ✅ None |
| Production ready | ❌ No | ✅ Yes |

### Example Output

**Query:** "What's the weather in Boston?"

**Base Model:**
```
1. functioncall> {"name": "get_weather"...
[Missing < tag, continues with hallucinated data]
```
❌ Not detected - malformed format

**Fine-tuned Model:**
```
<functioncall> {"name": "get_weather", "arguments": '{"location": "Boston"}'}
```
✅ Detected → MCP executes → Returns real weather data

## Key Learnings

### 1. When to Fine-tune

**✅ Fine-tune when:**
- Custom output formats (like Glaive)
- Domain-specific behavior (medical, legal, technical)
- Proprietary patterns
- Base model fails on your use case
- Need higher accuracy than base model

**❌ Don't fine-tune when:**
- Model has native support for your task
- Base model already achieves target accuracy
- You haven't tested the base model first

### 2. Format Matters

Three different formats encountered:

**Glaive (this project):**
```
<functioncall> {"name": "...", "arguments": '{"param": "value"}'}
```

**OpenAI/Industry Standard:**
```json
{"role": "assistant", "tool_calls": [{"type": "function", ...}]}
```

**Qwen Native (Qwen2.5-3B has this built-in!):**
```
<tool_call>{"name": "...", "arguments": {"param": "value"}}</tool_call>
```

### 3. MLX Training Optimizations

**Memory tricks for Apple Silicon:**
```yaml
batch_size: 1                    # Minimal memory per step
grad_accumulation_steps: 4       # Same gradient as batch=4
grad_checkpoint: true            # 30-50% memory savings
```

**LoRA configuration:**
```yaml
rank: 8                          # Sweet spot for efficiency
scale: 20.0                      # Alpha value (scale/rank = 2.5x)
keys: [q_proj, k_proj, v_proj, o_proj]  # Attention layers only
```

### 4. Dataset Conversion is Critical

**Bug we fixed:** Initial parser concatenated all messages into user role.

**Solution:** Regex-based splitting:
```python
parts = re.split(r'\n\n+(?=(?:USER:|ASSISTANT:|FUNCTION RESPONSE:))', chat)
```

**Always verify:** `head -1 data/train.jsonl | python -m json.tool`

### 5. Test Evaluation Performance

**Important:** MLX test evaluation is extremely slow (~7-8 hours for 1k samples).

**Recommendation:**
- Keep `test: false` in train/config.yaml
- Use validation loss during training
- Test with demo queries instead

## Deployment

### Local Testing

```bash
# Run demo (MCP server starts automatically)
uv run mcp/client.py
```

### Production Integration

The fine-tuned model + parser can integrate with:
- **Claude Desktop** - via MCP protocol
- **Custom applications** - using the MCP client code
- **Multiple MCP servers** - weather, databases, APIs, etc.

## Troubleshooting

### Out of Memory

- Reduce `batch_size` to 1
- Enable `grad_checkpoint: true`
- Reduce `num_layers` (try 4 instead of 8)
- Close other applications

### Poor Accuracy

- Increase `iters` (try 3000+)
- Increase sample size (40k+ recommended)
- Check dataset conversion (inspect train.jsonl)
- Verify `num_layers` is at least 8

### MCP Connection Issues

- Ensure `uv` is installed: `curl -LsSf https://astral.sh/uv/install.sh | sh`
- Check server starts correctly: `uv run mcp_server.py`
- Verify port availability

## Next Steps

### Extend the Project

1. **Add more MCP tools** - Just add `@mcp.tool()` decorated functions
2. **Connect multiple servers** - Modify client to connect to different MCP servers
3. **Deploy as service** - FastMCP supports HTTP/SSE transports
4. **Interactive CLI** - Add user input loop for chat-like experience

### Try Different Use Cases

Fine-tuning pipeline is reusable for:
- Structured data extraction (JSON, XML)
- Code generation in specific frameworks
- Domain-specific chat (medical, legal)
- Custom DSL generation (SQL, regex)
- Instruction following improvements

## Technical Stack

- **MLX** - Apple's ML framework for Apple Silicon
- **FastMCP** - Model Context Protocol server implementation
- **Qwen3-0.6B** - Small, efficient base model (600M params)
- **LoRA** - Parameter-efficient fine-tuning
- **Rich** - Beautiful terminal UI

## Results Summary

### What We Built

✅ Complete MLX LoRA fine-tuning pipeline
✅ Production-ready function calling agent
✅ MCP integration (no retraining needed)
✅ Side-by-side comparison demo
✅ Real API integrations (weather, currency)
✅ 100% accuracy on test queries

### What We Learned

✅ When fine-tuning is necessary vs when it's not
✅ Complete MLX training optimization for Apple Silicon
✅ MCP is protocol-agnostic (any model format works)
✅ Dataset quality and conversion is critical
✅ Small models (600M params) can achieve perfect accuracy

### Performance Metrics

- **Training time:** 1h 7m (40k samples)
- **Memory usage:** 5.5 GB peak
- **Inference latency:** 1-2s per query
- **Accuracy:** 100% (12/12 test queries)
- **Model size:** 130 MB (LoRA adapters)

## License

MIT

## Acknowledgments

- **MLX Team** at Apple for the excellent framework
- **Qwen Team** at Alibaba for the base model
- **Glaive AI** for the function calling dataset
- **Anthropic** for the Model Context Protocol specification
